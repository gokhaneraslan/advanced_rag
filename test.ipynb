{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a916b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import chromadb\n",
    "from typing import List, Optional, Literal\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain.retrievers.ensemble import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "LLMProvider = Literal[\"openai\", \"gemini\", \"groq\", \"ollama\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df10848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads a document from the given file path based on its extension (.txt or .pdf).\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the document to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of loaded LangChain Document objects.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the file extension is not .txt or .pdf.\n",
    "        FileNotFoundError: If the file is not found at the specified path.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found at: {file_path}\")\n",
    "\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "\n",
    "    if file_extension.lower() == '.txt':\n",
    "        loader = TextLoader(file_path, encoding='utf-8')\n",
    "    elif file_extension.lower() == '.pdf':\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: '{file_extension}'. Only '.txt' and '.pdf' are supported.\")\n",
    "    \n",
    "    print(f\"Loading '{os.path.basename(file_path)}'...\")\n",
    "    \n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(\"Loading complete.\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def load_documents_from_directory(directory_path: str) -> List[Document]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Loads all supported documents (.txt and .pdf) from a specified directory.\n",
    "\n",
    "    It iterates through all files in the given directory, identifies files\n",
    "    with '.txt' or '.pdf' extensions, and loads them using the appropriate\n",
    "    LangChain loader.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path to the directory to scan for documents.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A single list containing all loaded documents from the directory.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified directory does not exist.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.isdir(directory_path):\n",
    "        raise FileNotFoundError(f\"Directory not found at: {directory_path}\")\n",
    "\n",
    "    all_documents = []\n",
    "    supported_extensions = ['.txt', '.pdf']\n",
    "    \n",
    "    print(f\"Scanning directory '{directory_path}' for supported files ({', '.join(supported_extensions)})...\")\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        _, file_extension = os.path.splitext(filename)\n",
    "        \n",
    "\n",
    "        if os.path.isfile(file_path) and file_extension.lower() in supported_extensions:\n",
    "            \n",
    "            print(f\"  -> Found and loading '{filename}'...\")\n",
    "            \n",
    "            if file_extension.lower() == '.txt':\n",
    "                loader = TextLoader(file_path, encoding='utf-8')\n",
    "            elif file_extension.lower() == '.pdf':\n",
    "                loader = PyPDFLoader(file_path)\n",
    "            \n",
    "\n",
    "            try:\n",
    "                \n",
    "                loaded_docs = loader.load()\n",
    "                all_documents.extend(loaded_docs)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    [Warning] Failed to load or process {filename}: {e}\")\n",
    "\n",
    "    if not all_documents:\n",
    "        print(\"Warning: No supported documents were found in the directory.\")\n",
    "        \n",
    "    print(f\"Directory scan complete. Total documents loaded: {len(all_documents)}\")\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "\n",
    "\n",
    "def split_text(documents: List[Document], method: str = \"recursive\", **kwargs) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Splits the loaded documents according to the specified method.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): A list of LangChain Document objects.\n",
    "        method (str): The splitting method. Can be \"recursive\" or \"semantic\".\n",
    "        **kwargs: Method-specific arguments.\n",
    "            - For \"recursive\": `chunk_size` (int), `chunk_overlap` (int)\n",
    "            - For \"semantic\": `embeddings` (an instance of HuggingFaceEmbeddings)\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of split text chunks as LangChain Document objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting splitting process with '{method}' method...\")\n",
    "\n",
    "    if method == \"recursive\":\n",
    "        chunk_size = kwargs.get('chunk_size', 1000)\n",
    "        chunk_overlap = kwargs.get('chunk_overlap', 100)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        \n",
    "    elif method == \"semantic\":\n",
    "        embeddings = kwargs.get('embeddings')\n",
    "        \n",
    "        if embeddings is None:\n",
    "            raise ValueError(\"An 'embeddings' model is required for the semantic method.\")\n",
    "        \n",
    "        text_splitter = SemanticChunker(embeddings)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid method: '{method}'. Choose 'recursive' or 'semantic'.\")\n",
    "\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"The text was split into {len(texts)} chunks.\")\n",
    "    \n",
    "    return texts\n",
    "\n",
    "def create_or_load_chroma_retriever(\n",
    "\t\tpersist_directory: str, \n",
    "\t\tcollection_name: str,\n",
    "\t\tembeddings: HuggingFaceEmbeddings, \n",
    "\t\tdocuments: Optional[List[Document]] = None,\n",
    "\t\tsearch_type: str = \"mmr\",\n",
    "\t\tsearch_kwargs: dict = {\"k\": 5}\n",
    "\t) -> VectorStoreRetriever:\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates and persists a new Chroma vector store or loads an existing one.\n",
    "\n",
    "    This function checks if a collection with the given name already exists in the\n",
    "    persist_directory. If it does, it loads the store. If not, it creates a\n",
    "    new one using the provided documents.\n",
    "\n",
    "    Args:\n",
    "        persist_directory (str): The directory to save to or load from.\n",
    "        collection_name (str): The name for the collection within Chroma.\n",
    "        embeddings (HuggingFaceEmbeddings): The embedding model to use.\n",
    "        documents (Optional[List[Document]], optional): The list of split documents. \n",
    "            Required only if the collection does not already exist. Defaults to None.\n",
    "        search_type (str, optional): The type of search for the retriever. Defaults to \"mmr\".\n",
    "        search_kwargs (dict, optional): Keyword arguments for the search. Defaults to {\"k\": 5}.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreRetriever: A configured retriever for the Chroma vector store.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the collection does not exist and no documents are provided.\n",
    "    \"\"\"\n",
    "\n",
    "    client = chromadb.PersistentClient(path=persist_directory)\n",
    "    \n",
    "    existing_collections = [c.name for c in client.list_collections()]\n",
    "    \n",
    "    if collection_name in existing_collections:\n",
    "\n",
    "        print(f\"Collection '{collection_name}' found in '{persist_directory}'. Loading from disk.\")\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(f\"Collection '{collection_name}' not found. Creating a new one...\")\n",
    "        \n",
    "        if not documents:\n",
    "            raise ValueError(\n",
    "                \"Documents must be provided to create a new collection, but 'documents' parameter was None.\"\n",
    "            )\n",
    "        \n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings,\n",
    "            collection_name=collection_name,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        \n",
    "        print(f\"New collection created and persisted at '{persist_directory}'.\")\n",
    "\n",
    "    print(f\"Creating retriever with search_type='{search_type}' and search_kwargs={search_kwargs}.\")\n",
    "    \n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=search_type, \n",
    "        search_kwargs=search_kwargs\n",
    "    )\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "\n",
    "def create_bm25_retriever(\n",
    "\t\tdocuments: List[Document],\n",
    "\t\tk: int = 5\n",
    "\t) -> BM25Retriever:\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a BM25Retriever for keyword-based search from a list of documents.\n",
    "\n",
    "    BM25 is a ranking function that scores documents based on the query terms\n",
    "    appearing in each document, without using semantic understanding.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): The list of documents to index for keyword search.\n",
    "        k (int, optional): The number of documents to retrieve. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        BM25Retriever: A configured retriever for keyword search.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Creating BM25Retriever with k={k}...\")\n",
    "    \n",
    "    bm25_retriever = BM25Retriever.from_documents(\n",
    "        documents=documents,\n",
    "        k=k\n",
    "    )\n",
    "    \n",
    "    print(\"BM25Retriever created successfully.\")\n",
    "    \n",
    "    return bm25_retriever\n",
    "\n",
    "\n",
    "\n",
    "def create_ensemble_retriever(\n",
    "\t\tretrievers: List[BaseRetriever],\n",
    "\t) -> EnsembleRetriever:\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates an EnsembleRetriever to combine the results of multiple retrievers.\n",
    "    It uses Reciprocal Rank Fusion (RRF) to re-rank the combined results,\n",
    "    providing a more robust final ranking.\n",
    "\n",
    "    Args:\n",
    "        retrievers (List[BaseRetriever]): A list of retrievers to combine \n",
    "            (e.g., [chroma_retriever, bm25_retriever]).\n",
    "\n",
    "    Returns:\n",
    "        EnsembleRetriever: A retriever that combines and re-ranks results.\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    print(f\"Creating EnsembleRetriever for {len(retrievers)} retrievers...\")\n",
    "    \n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=retrievers,\n",
    "        fusion_type=\"RRF\",\n",
    "    \tc=60\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(\"EnsembleRetriever created successfully.\")\n",
    "    \n",
    "    return ensemble_retriever\n",
    "\n",
    "\n",
    "def create_compression_retriever(\n",
    "\t\tbase_retriever: BaseRetriever,\n",
    "\t\tembeddings: HuggingFaceEmbeddings,\n",
    "\t\treranker_model_name: str = \"BAAI/bge-reranker-large\",\n",
    "\t\ttop_n: int = 5,\n",
    "\t\tsimilarity_threshold: float = 0.95\n",
    "\t) -> ContextualCompressionRetriever:\n",
    "    \n",
    "    \"\"\"\n",
    "    Wraps a base retriever with a compression and reranking pipeline.\n",
    "\n",
    "    This pipeline enhances retrieval results by:\n",
    "    1. Filtering out redundant documents (semantically similar ones).\n",
    "    2. Reranking the remaining documents with a powerful Cross-Encoder model for relevance.\n",
    "    3. Reordering the documents to place the most relevant ones at the beginning and end,\n",
    "       combating the \"lost in the middle\" problem for Large Language Models.\n",
    "\n",
    "    Args:\n",
    "        base_retriever (BaseRetriever): The retriever to enhance (e.g., an EnsembleRetriever).\n",
    "        embeddings (HuggingFaceEmbeddings): The embedding model, needed for the redundant filter.\n",
    "        reranker_model_name (str, optional): The name of the Cross-Encoder model for reranking.\n",
    "        top_n (int, optional): The number of top documents to return after reranking. Defaults to 5.\n",
    "        similarity_threshold (float, optional): The threshold for filtering similar documents.\n",
    "                                                 Defaults to 0.95.\n",
    "\n",
    "    Returns:\n",
    "        ContextualCompressionRetriever: The enhanced retriever.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating advanced compression and reranking pipeline...\")\n",
    "\n",
    "\n",
    "    print(f\"Loading reranker model: {reranker_model_name}...\")\n",
    "    \n",
    "    reranker_model = HuggingFaceCrossEncoder(model_name=reranker_model_name)\n",
    "    compressor = CrossEncoderReranker(model=reranker_model, top_n=top_n)\n",
    "\n",
    "    redundant_filter = EmbeddingsRedundantFilter(\n",
    "        embeddings=embeddings,\n",
    "        similarity_threshold=similarity_threshold\n",
    "    )\n",
    "\n",
    "    reordering = LongContextReorder()\n",
    "\n",
    "    # The order is important: filter -> rerank -> reorder\n",
    "    pipeline_compressor = DocumentCompressorPipeline(\n",
    "        transformers=[redundant_filter, compressor, reordering]\n",
    "    )\n",
    "    \n",
    "    print(\"Compressor pipeline created successfully.\")\n",
    "\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=pipeline_compressor,\n",
    "        base_retriever=base_retriever\n",
    "    )\n",
    "    \n",
    "    print(\"ContextualCompressionRetriever created successfully.\")\n",
    "    \n",
    "    return compression_retriever\n",
    "\n",
    "\n",
    "\n",
    "def initialize_llm(\n",
    "\t\tprovider: LLMProvider,\n",
    "\t\tmodel_name: str = None,\n",
    "\t\t**kwargs\n",
    "\t) -> BaseChatModel:\n",
    "    \n",
    "    \"\"\"\n",
    "    Initializes and returns a LangChain Chat Model from a specified provider.\n",
    "\n",
    "    This function acts as a factory for different LLM providers, loading the\n",
    "    necessary API keys from environment variables.\n",
    "\n",
    "    Args:\n",
    "        provider (Literal[\"openai\", \"gemini\", \"groq\", \"ollama\"]):\n",
    "            The LLM provider to use.\n",
    "        model_name (str, optional): The specific model to use from the provider.\n",
    "            If None, a sensible default will be used for each provider.\n",
    "        **kwargs: Additional keyword arguments to pass to the model's constructor\n",
    "                  (e.g., temperature=0.7, max_tokens=1024).\n",
    "\n",
    "    Returns:\n",
    "        BaseChatModel: An instance of the requested LangChain chat model.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unsupported provider is specified or if the required\n",
    "                    API key environment variable is not set.\n",
    "    \"\"\"\n",
    "    \n",
    "    provider = provider.lower()\n",
    "    \n",
    "    print(f\"Initializing LLM from provider: '{provider}'...\")\n",
    "\n",
    "    if provider == \"gemini\":\n",
    "        \n",
    "        if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "            raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "        \n",
    "        model = model_name or \"gemini-2.5-flash\"\n",
    "        \n",
    "        return ChatGoogleGenerativeAI(model=model, **kwargs)\n",
    "    \n",
    "    elif provider == \"openai\":\n",
    "        \n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
    "        \n",
    "        model = model_name or \"gpt-4o\"\n",
    "        \n",
    "        return ChatOpenAI(model=model, **kwargs)\n",
    "\n",
    "    elif provider == \"groq\":\n",
    "        \n",
    "        if not os.getenv(\"GROQ_API_KEY\"):\n",
    "            raise ValueError(\"GROQ_API_KEY environment variable not set.\")\n",
    "        \n",
    "        model = model_name or \"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
    "        \n",
    "        return ChatGroq(model_name=model, **kwargs)\n",
    "\n",
    "\n",
    "    elif provider == \"ollama\":\n",
    "        \n",
    "        model = model_name or \"llama3\"\n",
    "        \n",
    "        print(f\"Note: Ensure the Ollama service is running and you have pulled the '{model}' model.\")\n",
    "        \n",
    "        return ChatOllama(model=model, **kwargs)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        raise ValueError(\n",
    "            f\"Unsupported LLM provider: '{provider}'. \"\n",
    "            \"Supported providers are 'openai', 'gemini', 'groq', 'ollama'.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def create_rag_chain(\n",
    "    retriever: BaseRetriever, \n",
    "    llm: BaseChatModel, \n",
    "    prompt_template: Optional[str] = None\n",
    "\t) -> Runnable:\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a Retrieval-Augmented Generation (RAG) chain.\n",
    "\n",
    "    This chain orchestrates the entire process:\n",
    "    1. It takes a user's question.\n",
    "    2. It uses the provided retriever to fetch relevant documents.\n",
    "    3. It stuffs the documents and the question into a prompt.\n",
    "    4. It sends the prompt to the LLM to generate an answer.\n",
    "\n",
    "    Args:\n",
    "        retriever (BaseRetriever): The configured retriever instance \n",
    "            (e.g., the final compression retriever).\n",
    "        llm (BaseChatModel): The initialized language model.\n",
    "        prompt_template (Optional[str], optional): A custom system prompt template string.\n",
    "            Must include a '{context}' placeholder. If None, a default prompt is used.\n",
    "\n",
    "    Returns:\n",
    "        Runnable: A LangChain runnable object that can be invoked with a query.\n",
    "                  The output is a dictionary containing \"input\", \"context\", and \"answer\".\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating the final RAG chain...\")\n",
    "\n",
    "\n",
    "    if prompt_template is None:\n",
    "        \n",
    "        prompt_template = (\n",
    "            \"You are an assistant for question-answering tasks. \"\n",
    "            \"Use the following pieces of retrieved context to answer the question. \"\n",
    "            \"If you don't know the answer, just say that you don't know. \"\n",
    "            \"Keep the answer concise and based ONLY on the provided context.\\n\\n\"\n",
    "            \"CONTEXT:\\n{context}\"\n",
    "        )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", prompt_template),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "    \n",
    "    print(\"RAG chain created successfully.\")\n",
    "    \n",
    "    return retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    test_file_path = \"./data/test.pdf\"\n",
    "    embeddings_model = None\n",
    "    \n",
    "    try:\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Using '{device}' for the embedding model...\")\n",
    "        \n",
    "        embeddings_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "            model_kwargs={'device': device},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        \n",
    "        print(\"Embedding model loaded successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the embedding model: {e}\")\n",
    "        print(\"Semantic splitting test will be skipped.\")\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        loaded_documents = load_document(test_file_path)\n",
    "        \n",
    "        print(f\"A total of {len(loaded_documents)} document(s) were loaded.\\n\")\n",
    "\n",
    "\n",
    "        print(\"--- Recursive Method Test ---\")\n",
    "        \n",
    "        recursive_chunks = split_text(\n",
    "            loaded_documents, \n",
    "            method=\"recursive\", \n",
    "            chunk_size=1000, \n",
    "            chunk_overlap=100\n",
    "        )\n",
    "        \n",
    "        print(\"\\nFirst recursive chunk:\\n\", recursive_chunks[0].page_content)\n",
    "        print(\"-\" * 25)\n",
    "\n",
    "        \n",
    "        if embeddings_model:\n",
    "            print(\"\\n--- Semantic Method Test ---\")\n",
    "            \n",
    "            semantic_chunks = split_text(\n",
    "                loaded_documents, \n",
    "                method=\"semantic\", \n",
    "                embeddings=embeddings_model\n",
    "            )\n",
    "            \n",
    "\n",
    "            print(\"\\nFirst semantic chunk:\\n\", semantic_chunks[0].page_content)\n",
    "            print(\"-\" * 25)\n",
    "\n",
    "\n",
    "\n",
    "        db_directory = \"./chroma_db_store\"\n",
    "        collection = \"test_collection\"\n",
    "        \n",
    "        vector_retriever = create_or_load_chroma_retriever(\n",
    "\t\t\tpersist_directory=db_directory,\n",
    "\t\t\tcollection_name=collection,\n",
    "\t\t\tembeddings=embeddings_model,\n",
    "\t\t\tdocuments=semantic_chunks,\n",
    "   \t\t\tsearch_type = \"mmr\",\n",
    "\t\t\tsearch_kwargs={\"k\": 5}\n",
    "\t\t)\n",
    "        \n",
    "        keyword_retriever = create_bm25_retriever(\n",
    "\t\t\tdocuments=semantic_chunks,\n",
    "\t\t\tk=5\n",
    "\t\t)\n",
    "        \n",
    "        ensemble_retriever = create_ensemble_retriever(\n",
    "\t\t\tretrievers=[vector_retriever, keyword_retriever]\n",
    "\t\t)\n",
    "        \n",
    "        final_retriever = create_compression_retriever(\n",
    "\t\t\tbase_retriever=ensemble_retriever,\n",
    "\t\t\tembeddings=embeddings_model,\n",
    "\t\t\ttop_n=5\n",
    "\t\t)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "        \n",
    "        query = \"What is a Semantic Analysis?\"\n",
    "        \n",
    "        print(f\"Testing FINAL retriever with query: '{query}'\")\n",
    "        \n",
    "        final_docs = final_retriever.invoke(query)\n",
    "        \n",
    "        print(\"\\n--- Final Retrieved & Reranked Documents ---\")\n",
    "        \n",
    "        for i, doc in enumerate(final_docs):\n",
    "            print(f\"Document {i+1}: {doc.page_content} (Source: {doc.metadata.get('source')})\")\n",
    "            \n",
    "        \n",
    "        simple_query = \"Hello! Is there anybody there?\"\n",
    "        \n",
    "        if os.getenv(\"OPENAI_API_KEY\"):\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                print(\"\\n--- Testing OpenAI ---\")\n",
    "                llm_openai = initialize_llm(\n",
    "                    \"openai\", \n",
    "                    model_name=\"gpt-4o\", \n",
    "                    temperature=0\n",
    "                )\n",
    "                \n",
    "                response = llm_openai.invoke(simple_query)\n",
    "                \n",
    "                print(\"OpenAI Response:\\n\", response.content)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error testing OpenAI: {e}\")\n",
    "                \n",
    "        else:\n",
    "            print(\"\\nSkipping OpenAI test: OPENAI_API_KEY not set.\")\n",
    "        \n",
    "        if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                print(\"\\n--- Testing Google Gemini ---\")\n",
    "                \n",
    "                llm_gemini = initialize_llm(\n",
    "\t\t\t\t\t\"gemini\", \n",
    "\t\t\t\t\tmodel_name=\"gemini-2.5-flash\", \n",
    "\t\t\t\t\ttemperature=0\n",
    "                )\n",
    "                \n",
    "                response = llm_gemini.invoke(simple_query)\n",
    "                \n",
    "                print(\"Gemini Response:\\n\", response.content)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error testing Gemini: {e}\")\n",
    "        \n",
    "        else:\n",
    "            print(\"\\nSkipping Gemini test: GOOGLE_API_KEY not set.\")\n",
    "            \n",
    "        if os.getenv(\"GROQ_API_KEY\"):\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                print(\"\\n--- Testing Groq ---\")\n",
    "                \n",
    "                llm_groq = initialize_llm(\n",
    "\t\t\t\t\t\"groq\", \n",
    "\t\t\t\t\ttemperature=0\n",
    "                )\n",
    "                \n",
    "                response = llm_groq.invoke(simple_query)\n",
    "                print(\"Groq Response:\\n\", response.content)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error testing Groq: {e}\")\n",
    "                \n",
    "        else:\n",
    "            print(\"\\nSkipping Groq test: GROQ_API_KEY not set.\")\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            print(\"\\n--- Testing Ollama ---\")\n",
    "            \n",
    "            llm_ollama = initialize_llm(\n",
    "\t\t\t\t\"ollama\", \n",
    "\t\t\t\tmodel_name=\"llama3\", \n",
    "\t\t\t\ttemperature=0\n",
    "            )\n",
    "            \n",
    "            response = llm_ollama.invoke(simple_query)\n",
    "            \n",
    "            print(\"Ollama Response:\\n\", response.content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not connect to Ollama. Please ensure the service is running and the model is pulled.\")\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        \n",
    "        rag_chain = create_rag_chain(\n",
    "\t\t\tretriever=final_retriever,\n",
    "\t\t\tllm=llm_gemini\n",
    "   \t\t)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "        \n",
    "        query = \"What is Lexical Analysis?\"\n",
    "        print(f\"Invoking chain with query: '{query}'\")\n",
    "        \n",
    "        response = rag_chain.invoke({\"input\": query})\n",
    "        \n",
    "        print(\"\\n--- Full Response Dictionary ---\")\n",
    "        print(response)\n",
    "        \n",
    "        print(\"\\n--- Retrieved Context ---\")\n",
    "        \n",
    "        for doc in response[\"context\"]:\n",
    "            print(doc.page_content)\n",
    "            \n",
    "        print(\"\\n--- Final Answer ---\")\n",
    "        print(response[\"answer\"])\n",
    "\n",
    "    except (FileNotFoundError, ValueError, RuntimeError) as e:\n",
    "        print(f\"An error occurred during the workflow: {e}\")\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
